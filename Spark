1)Spark internal Architecture?

2)Repartition and colease

3)Cache and Persist

4)Difference between CSV JSON Parquest and Avro

5)Driver and Executor OOM

6)groupby key vs reduce by key. Which one is optimal and why ?

7)Transformation(filter,union) and Action(showw,collect,take,count,save) 

8)External Shuffle Service why is it used ?

8)Dynamic Resource Allocation why do we need Shuffle service ?

9)Spark App lifecycle->Spark submit->Driver Initiates Spark session->DAG creates logical plan ->Task Executor requests cluster manager for resources->Cluster manager allocates the resources -> Driver establishes connection with worker and assigns task->Worker executes the task and returs result to the driver ->driver returns result to the User ->App end 

10)lineage and DAG

11)catalyst Optimization

12)RDD dataframe vs Dataset

13)Driver and Worker Process ? These are JVM Process , one worker node has many executors ,each executor runs its own JVM Process

14)Spark submit ? -submit spark job
spark-submit --master Yarn --deployment-mode Client --executor-memory 4g --driver-memory 3g --num-executors 10 <.py>
local machine--spark-submit \
  --master local[4] \
  --executor-memory 2g \
  my_script.py

15)Application? Single spark code or multiple notebooks with complex logic

16)job-after app submit . Driver converts code to Job

17)stage - Job are divided into stages , based on the wide transform shuffle . For every shuffle a new stage is created 

18)Task - each task process 1 partition at a time . no of partition determine number of task 

19)RDD 

20)DAG -- keeps track of all transformation . For each transformation logical plan is created and lineage graph is maintained by dag

21)Executor - can be config by spark settings 

22)partition
23)core - threads

24)on heap memory -the executor memory on the jvm ,managed by jvm

25)off heap memory -the executor memory lies outside Jvm process manages by os

26)ibraries supported by spark -sql,stream,ml,graphx 

27)Driver architecture --tf->Dag scheduler ->Task Scheduler->Cluster Manager ->Deploye Scheduler to workers ->worker result given to driver tracked by out put tracker 

28) Worker Node Architecture ->executor -> cores->onheap memory (only one per executor) 
 all the above also connects to off heap memeory 

29)Advantage of ooff heap memory? we donot need to serialize and deserialize  ..........  Garbage collector -when memory full, gc scans entire memory and removes obsolute objects . This is unecessary process . This is present in off heap memory 

30)On heap memory architecture --see the image.png

31)calculate the number of cores and memory required (ex- 6 machines , 16 core , 64GB Memory)

32)Narrow(No shuffle,faster and efficient)-map,filter,union,mappartitiions,sample vs Wide Transformation (shuffle happens group by key , reduce by key ,join)

33)Explain Storage layer of onheap memory
Reserver Memory-Reserved by spark for internal purpose 
user Memory-storing the data structures created and managed by users code
Execution memory-jvm heap use full for shuffle operations
Storage Memory-JVM Heap for cache data 
dis: take more time to execute as gc will also pause the process to remove the objects

34)Explain off heap memory
if onheap cannot accomodate take help of off heap 
enable off heap by spark.memory.offHeap.use to true 
Accessing off heap is slower than the on heap but faster than read/write from disk .
Garbage collection scan is avoided .

(young generation move data to->old generation )

35)Cluster?
Databricks cluster is a set of computational resources and configurations on which you run DE, DS and Da workloads such as ETL Pipelines ,Stream Analytics and adhoc ml models .
present in compute . can conf min and max worker nodes
36)types of cluster 
All purpose CLusters:
used for developement . we can see output interactively 
Terminated ,restared manually.
Computing resource is share .

Job Cluster:
can run fast and robust automated jobs .Created automatically and terminates after end of excution 
Use for scheduled jobs

pools:
predefine the node count when attaching the cluster .
less booting time . Suitable for cost saving and larger teams .

37)Cluster modes ?
Standard:suitable for single user and if no team collaboration is required 
High Concurrency: suitable for collaboration. Provides fine grain sharing for max resources and min latency
Single Node: this mode runs job only on driver node and no worker node are provisioned .

38)Cluster runtime?
Runtime is set of core components that are needed for cluster to run 

39)spot instance?
workload of less priority. Azure check free instance(lesscost) uses this

40)Worker Types and driver type?
compute optimize
general 
memory
storage optimized

Cluster features :
logging into dbfs
tag
spark config

41)Reading CSV File
df=spark.read.format()
.option("inferSchema")
.option("header")
.option("sep")
.schema()
.load(path1,path2)

42)what is inferSchema?
Automatic Type Detection: It can automatically infer the data types of columns such as String, Integer, Double, Date, etc.
Costly Operation
Default Behavior: By default, Spark assumes that the data is of type String for each column if inferSchema is not enabled.

43)
