1)Spark internal Architecture?

2)Repartition and colease

3)Cache and Persist

4)Difference between CSV JSON Parquest and Avro

5)Driver and Executor OOM

6)groupby key vs reduce by key. Which one is optimal and why ?

7)Transformation(filter,union) and Action(showw,collect,take,count,save) 

8)External Shuffle Service why is it used ?

8)Dynamic Resource Allocation why do we need Shuffle service ?

9)Spark App lifecycle->Spark submit->Driver Initiates Spark session->DAG creates logical plan ->Task Executor requests cluster manager for resources->Cluster manager allocates the resources -> Driver establishes connection with worker and assigns task->Worker executes the task and returs result to the driver ->driver returns result to the User ->App end 

10)lineage and DAG

11)catalyst Optimization

12)RDD dataframe vs Dataset

13)Driver and Worker Process ? These are JVM Process , one worker node has many executors ,each executor runs its own JVM Process

14)Spark submit ? -submit spark job
spark-submit --master Yarn --deployment-mode Client --executor-memory 4g --driver-memory 3g --num-executors 10 <.py>
local machine--spark-submit \
  --master local[4] \
  --executor-memory 2g \
  my_script.py

15)Application? Single spark code or multiple notebooks with complex logic

16)job-after app submit . Driver converts code to Job

17)stage - Job are divided into stages , based on the wide transform shuffle . For every shuffle a new stage is created 

18)Task - each task process 1 partition at a time . no of partition determine number of task 

19)RDD 

20)DAG -- keeps track of all transformation . For each transformation logical plan is created and lineage graph is maintained by dag

21)Executor - can be config by spark settings 

22)partition
23)core - threads

24)on heap memory -the executor memory on the jvm ,managed by jvm

25)off heap memory -the executor memory lies outside Jvm process manages by os

26)ibraries supported by spark -sql,stream,ml,graphx 

27)Driver architecture --tf->Dag scheduler ->Task Scheduler->Cluster Manager ->Deploye Scheduler to workers ->worker result given to driver tracked by out put tracker 

28) Worker Node Architecture ->executor -> cores->onheap memory (only one per executor) 
 all the above also connects to off heap memeory 

29)Advantage of ooff heap memory? we donot need to serialize and deserialize  ..........  Garbage collector -when memory full, gc scans entire memory and removes obsolute objects . This is unecessary process . This is present in off heap memory 

30)On heap memory architecture --see the image.png

31)calculate the number of cores and memory required (ex- 6 machines , 16 core , 64GB Memory)

32)Narrow(No shuffle,faster and efficient)-map,filter,union,mappartitiions,sample vs Wide Transformation (shuffle happens group by key , reduce by key ,join)

33)Explain Storage layer of onheap memory
Reserver Memory-Reserved by spark for internal purpose 
user Memory-storing the data structures created and managed by users code
Execution memory-jvm heap use full for shuffle operations
Storage Memory-JVM Heap for cache data 
dis: take more time to execute as gc will also pause the process to remove the objects

34)Explain off heap memory
if onheap cannot accomodate take help of off heap 
enable off heap by spark.memory.offHeap.use to true 
Accessing off heap is slower than the on heap but faster than read/write from disk .
Garbage collection scan is avoided .

(young generation move data to->old generation )

35)Cluster?
Databricks cluster is a set of computational resources and configurations on which you run DE, DS and Da workloads such as ETL Pipelines ,Stream Analytics and adhoc ml models .
present in compute . can conf min and max worker nodes
36)types of cluster 
All purpose CLusters:
used for developement . we can see output interactively 
Terminated ,restared manually.
Computing resource is share .

Job Cluster:
can run fast and robust automated jobs .Created automatically and terminates after end of excution 
Use for scheduled jobs

pools:
predefine the node count when attaching the cluster .
less booting time . Suitable for cost saving and larger teams .

37)Cluster modes ?
Standard:suitable for single user and if no team collaboration is required 
High Concurrency: suitable for collaboration. Provides fine grain sharing for max resources and min latency
Single Node: this mode runs job only on driver node and no worker node are provisioned .

38)Cluster runtime?
Runtime is set of core components that are needed for cluster to run 

39)spot instance?
workload of less priority. Azure check free instance(lesscost) uses this

40)Worker Types and driver type?
compute optimize
general 
memory
storage optimized

Cluster features :
logging into dbfs
tag
spark config

41)Reading CSV File
df=spark.read.format("")
.option("inferSchema","true")
.option("header","true")
.option("sep",",")
.schema(schema)
.load(path1,path2)

42)what is inferSchema?
Automatic Type Detection: It can automatically infer the data types of columns such as String, Integer, Double, Date, etc.
Costly Operation
Default Behavior: By default, Spark assumes that the data is of type String for each column if inferSchema is not enabled.

43)what is printSchema()?syntax

44)crud:
add:df.withColumn("new_column_name",Value)
rename: df.withColumnRenamed("Oldname","New_name")
drop:df.drop("column Name")

45)Add new Column:
df.withColumn("location",lit("Mumbai")).show()

46)How to add newColumn by calculation
df.withColumn("Bonus",emDf.salary*0.1)

47)join
# Joining on multiple columns
df1 = spark.createDataFrame([(1, "Alice", "A"), (2, "Bob", "B"), (3, "Charlie", "C")], ["id", "name", "group"])
df2 = spark.createDataFrame([(1, "Math", "A"), (2, "Science", "B"), (4, "History", "D")], ["id", "subject", "group"])

result = df1.join(df2, on=["id", "group"], how="inner")
result.show()

from pyspark.sql.functions import broadcast
result = df1.join(broadcast(df2), on="id", how="inner")

Join Type	Description
inner	Returns only matching rows from both DataFrames (default join type).
left	Returns all rows from the left DataFrame, matching rows from the right.
right	Returns all rows from the right DataFrame, matching rows from the left.
outer	Returns all rows from both DataFrames, with null values where no match.
left_semi	Returns only rows from the left DataFrame that have a matching key in the right.
left_anti	Returns only rows from the left DataFrame that do not have a matching key in the right.

48)DBUtils-
Databricks Utils help to interact with databricks file and Notebooks.
handle i/p and o/p of notebooks

dbutils.fs.help()
dbutils.notebook.help()
dbutils.widgets.help()
dbutils.secrets.help("get")
dbutils.fs.help("cp")
dbutils.notebook.help("exit")


49)Dynamic Partition Pruning / Predicate push down 
50)Handling Skewed data
51)Spark Optimizations 
52)Partitio Vs Buckets
53) workspace in ADB 
54)Assets of Workspace in ADB 
55) how to access work space?
56)notebooks ?
57)Job ? Job is a mechanism to run code in ADB  we can automate this 
58)library?
59)Data in ADB?
60)Experiments?
61)Workflows?
Azure Databricks (ADB), a workflow refers to the process of organizing and executing a series of tasks, notebooks, or jobs that are intended to accomplish a particular data engineering, data science, or machine learning process.
Databricks provides several tools to help define, schedule, and automate workflows efficiently, from simple batch processing to complex data pipelines.

Example of a Databricks Workflow
Consider a simple data pipeline where you:

Extract data from a source (e.g., a CSV file in Azure Blob Storage).
Transform the data using a series of PySpark transformations.
Load the transformed data into a Delta Lake table for further processing.
This process can be broken down into several tasks:

Task 1: Run a notebook that extracts data from Azure Blob Storage and performs some basic cleaning.
Task 2: Run a second notebook to perform more complex transformations on the data.
Task 3: Save the transformed data to Delta Lake, ensuring that data integrity is maintained.
In the job configuration, you can set the task dependencies so that Task 2 will not start until Task 1 finishes, and Task 3 will start only after Task 2.

62)Folder ?static assets 
63)special folder ? worspace shared user, cannot move or rename a folder 
64)flatten json:
df=spark.read.option("multiline","true").json("path")
df_flat = df.select(
    "name",
    F.col("address.street").alias("address_street"),
    F.col("address.city").alias("address_city"),
    F.col("address.zip.code").alias("address_zip_code"),
    F.col("address.zip.area").alias("address_zip_area"),
    F.explode("phones").alias("phone")
)

# Now flatten the 'phone' struct into separate columns
df_flat = df_flat.select(
    "name",
    "address_street",
    "address_city",
    "address_zip_code",
    "address_zip_area",
    F.col("phone.type").alias("phone_type"),
    F.col("phone.number").alias("phone_number")
)

df_flat.show(truncate=False)

65)DBFS
66)Mount, unmount,refreshMounts-dbsutil.fs.mount(source=,mount_point=,extra_config=)
67)run:dbutils.notebook.run()
68)exit:dbutils.notebook.exit()
69)Widgets- combox,dropdown,get,getArgument,multiselect,remove,text
70)pass parameter values from one notebook to another 
71)Parameterize SQl notebook-getArgument()
72)how to mount Azure blob storage ?1)Accountkey and sas token
73)what is account key in Azure
the Account Key typically refers to a unique key that is used to authenticate and access resources in Azure storage services such as Azure Blob Storage, Azure Queue Storage, Azure Table Storage, and Azure File Storage

74)In Azure, a Shared Access Signature (SAS) token provides a way to grant restricted access to resources in Azure Storage without exposing the account key
75)updateMount()
76)sparkSession vs sparkContext-entry to spark cluster
77)AQE-Coleases shuffle partition
-switching join strategies
-optimizing skew joins
78)checkpointing
79)serialize and deserialization obj to byte
80)Read Modes in pyspark
Failfast mode
dropmalformed mode
permissive mode-stores in "_corrupt_record"

.option("mode","DROPMALFORMED")

81)how to rename 100 columns in pyspark
# Define a dictionary with old column names as keys and new column names as values
rename_dict = {f"col{i}": f"new_col{i}" for i in range(1, 101)}

# Rename columns using the dictionary
for old_name, new_name in rename_dict.items():
    df = df.withColumnRenamed(old_name, new_name)

# Show the result
df.show(5)

82)how would you read multiple files with delimiters
83)you have a dataset of 500gb that needs processing on spark cluster with 10 nodes .Each node has 64gb of memory and 16 cores?--conf
84)Explain main features in deltalake that are adv
85)how delta lake supports acid?
86)Architecture
Architecture	Focus	Strengths	Weaknesses	Comparison with Medallion
Lambda	Real-time + batch processing	Handles real-time and historical data	Complex, dual-codebase	Medallion is simpler, focuses on quality improvement.
Kappa	Streaming data processing	Simplifies real-time processing	Limited historical data support	Medallion supports batch + streaming.
Data Vault	Data modeling for warehouses	Scalable, auditable	Complex modeling and queries	Medallion better for lakehouses.
Kimball	Dimensional modeling	Easy for BI and reporting	Rigid, lacks support for lakes	Medallion supports unstructured data.
Delta Lake	Lakehouse foundation	ACID transactions, time travel	Platform-dependent	Delta Lake underpins Medallion.
Lakehouse	Unified data storage	Combines lakes and warehouses	Still evolving	Medallion organizes lakehouse data.

87) SCD Type-2 (on data lake)-capture effective start and end date

customer_dim_data = [

(1,'manish','arwal','india','N','2022-09-15','2022-09-25'),
(2,'vikash','patna','india','Y','2023-08-12',None),
(3,'nikita','delhi','india','Y','2023-09-10',None),
(4,'rakesh','jaipur','india','Y','2023-06-10',None),
(5,'ayush','NY','USA','Y','2023-06-10',None),
(1,'manish','gurgaon','india','Y','2022-09-25',None),
]

customer_schema= ['id','name','city','country','active','effective_start_date','effective_end_date']

customer_dim_df = spark.createDataFrame(data= customer_dim_data,schema=customer_schema)

sales_data = [

(1,1,'manish','2023-01-16','gurgaon','india',380),
(77,1,'manish','2023-03-11','bangalore','india',300),
(12,3,'nikita','2023-09-20','delhi','india',127),
(54,4,'rakesh','2023-08-10','jaipur','india',321),
(65,5,'ayush','2023-09-07','mosco','russia',765),
(89,6,'rajat','2023-08-10','jaipur','india',321)
]

sales_schema = ['sales_id', 'customer_id','customer_name', 'sales_date', 'food_delivery_address','food_delivery_country', 'food_cost']

sales_df = spark.createDataFrame(data=sales_data,schema=sales_schema)

#left join to capture data change
joined_data=customer_dim_df.join(sales_df,customer_dim_df['id']==sales_df['customer_id'],"left")
new_records_df=join_data.where((col("food_delivery_address")!=col("city"))&(col("activite")=="Y"))
.withColums("active",lit("Y"))
.withColumn("effective_end_date",col("sales_date"))\
.withColum("effective_end_date",lit(None))
.select(
"cust_id",
Cust_name,
City,
food_delivery_country,
Active,
effective_start_date
effective_end_date)
)
#update Old record
old_record=joined_date.where(
(col("food_delivery_address"!=col("city"))&(col("active")=="Y"))
.witColumn("active","N")
.withColumn("effective_end_date",col("sales_date"))
.select()
)
#find out new customer
new_customer=sales_df.join(
customer_dim_df,sales_df["customer_id"]==customer_dim_df["id"],"leftanti")\
.withCOlumn('Active",lit("Y"))
.WitchCOlumn("effective_start_date",col("sales_date"))
.withCOlumn("effective_end_date",lit(None))
.select())
)

final_record=customer_dim_df.union(new_records_df).union(old_records).union(new_customer)

# remove duplicate

window=Window.partitionBy("id","active").orderBy(col("effective_start_date").desc())
final_record.withColumn("rnk",rank().over(window)).filter(~((col("active")=="Y")&(col("rank")>2))).drop("rnk")))


Data Engineer Interview!!

Interviewer: You're running a Spark job on a large dataset stored in HDFS. Suddenly, you notice that the job is taking longer than usual to complete. How would you troubleshoot ?

Candidate: When faced with a slowdown in a Spark job, several factors could be contributing to the issue. Here's how I would approach:

Interviewer: What would be your initial steps in troubleshooting the slow Spark job?

I would check the Spark UI to gather information about the job's execution, including task progress, stage durations, and resource utilization. This can help identify bottlenecks and performance issues within the job.

Interviewer: Could you provide some specific metrics or indicators you'd look for in the Spark UI?

I'd focus on metrics such as task duration, shuffle read/write times, executor CPU and memory utilization, and garbage collection activity. These metrics can provide insights into potential performance bottlenecks, such as data skew, resource contention, or inefficient task execution.

Interviewer: If you notice high shuffle read/write times in the Spark UI, how would you investigate further?

High shuffle read/write times often indicate issues with data skew or inefficient shuffle operations. I would drill down into the stage details in the Spark UI to identify tasks with disproportionately high shuffle read/write times. Analyzing the data distribution and partitioning strategy can help pinpoint the cause of the skew and optimize the shuffle operations accordingly.

Interviewer: What steps would you take if you suspect resource contention as the cause of the slowdown?

If resource contention is suspected, I would examine executor CPU and memory utilization to identify any resource bottlenecks. Increasing executor memory or adjusting the number of executors can help alleviate resource contention and improve job performance. Additionally, optimizing resource allocation and task scheduling parameters in the Spark configuration can further optimize resource utilization.

Interviewer: Suppose you've optimized resource utilization, but the job is still running slower than expected. What other factors would you consider?

If resource utilization is optimized, I would investigate other potential factors impacting job performance, such as inefficient data processing logic, data skew, or suboptimal partitioning strategies. Analyzing the job's DAG and execution plan can provide insights into the data processing flow and identify opportunities for optimization.

Interviewer: How would you ensure the stability and reliability of the Spark job after troubleshooting and optimization?

After troubleshooting and optimization, I would conduct thorough testing to validate the stability and reliability of the Spark job under varying workload conditions and data scenarios. This includes performance testing, stress testing, and fault tolerance testing to ensure the job performs reliably and efficiently in production environments.
--------------------------------------------------------------------------
df.rdd.getNumPartitions()
df1=df.select(spark_partition_id().alias('partid')).groupBy('partid').count()
-----------------------------------------------------------------------------

how to process files those are received before/after specified time?
option(modifiedBefore,modifiedAfter)
modifiedBefore- This attribute can be used to readfiles that were modified before the specified timestamp
modifiedAfter-This attribute can be used to read files that were modified after the specified timestamp .
df1=spark.read.option("header",True).csv(path="/mnt/input/Sales".csv,
                modifiedAfter="2024-01-30T00:00:00")

-------------------------------------------------------------------------------------

How to handle corrupted records
columnNameOfCorruptRecord-Allows rename the new filed having malformed string created by Premissive mode .
spark.read.option("header",True).schema(schema).option("mode","PREMISSIVE").csv(path="/mnt/input/Employee.csv",columnNameOfCorruptRecord="errorrows")

----------------------------------------------------------------------------------------
How to load only correct records while reading data from file ?
option mode(PERMISSIVE,DROPMALFORED,FAILFAST)
DROPMALFORED-ignores the whole corrupted record.
FastFail-throws an exception when it meets corrupted record
PERMISSIVE-put the malformed String into field configured by columnNameof CorruptedRecord and sets malformed fields to null

---------------------------------------------------------------------------------------------

how to create any separator in file writing & reading  in pyspark?
how to use linesep in file creation & reading in pyspark?
how to use inferschema while reading data from file in pyspark?

linesep=This attribute can be used to specify single as a separate for each row while reading and writing file using either option or options.
df.write.option("header","True").option("inferSchema","True").mode("overwrite").csv(path="",sep="|",linSep='/r')
delimiter: This attribute can be used to specify single/multiple chracters as a seperated for each,column which reaading or writing using with option or options function.
infer schema: it will automatically guess the dataatypes for each field , if we set option to true.
--------------------------------------------------------------------------------------------------------

How to read file from folder and subfolders?How to create zip/unzip files?
options(recursiveFileLookup,codec)
recursiveFileLookup: This attribute used to recursively scan the directories to read files(it will read data from subfolders too)
codec:This attribute can be used to compress csv or other delimited files using passed compression method .it only works on csv or normal delimited files . Spark can read gzip without specifying codec , but for writing codec must be specified .
spark.read.option("header",True).option("recursiveFileLookup",True).csv(path)
df.write.option("header",True).mode("overwrite").csv(path=,sep="|",compression="gzip",encoding="cp1252")

-------------------------------------------------------------------------------

can you provide overview of experience working with pyspark
How pyspark relate to apache saprk what adv does it offer in distributed data processing ?
provide examples of pyspark DF opertions you frequently used ?
Explain how data serialization works in pyspark
significance of choosing right compression codec
how do you deal with missing or null values
specific strategies or function you prefer for handling missing data ?
Experience with Spark SQL
How to execute sql queries on pyspark dataframes
provide an example where broad cast join can significantly improve performance ?
Describe experience with pyspark MLilb?
ML algo implemented using pysaprk
how do you monitor and trouble shoot spark jobs 
importance of logging in pyspark?
have you integrated pyspark with other big data thech or dbs
how do you handle data transfer between pyspark and external system 
challenges faced in project 
experience with cluster management in pyspark
How do you scale pyspark app in a cluster enviroment 
can you briefly describe some popular libraries or tools in pyspark ecosysytem apart from the core pyspark functionality?
How does pyspark integrate with other components of the hadoop ecosystem , such as HDFS and YARN?
Describe a scenario where you would use the pyspark dataframe API over RDDs
How can you manage and optimize the memory usage of you pyspark application?
How would you perform iterative operation in pyspark and its challenges?
how can you manage and optimize the memory usage of your pyspark application?
What is default file format in spark?
rack awareness in hadoop?
fl in spark?
how to handle redundant data in pyspark
how would you handle and ignore null values while loading data?
how would you find 3rd highest salary in dataset?
given a dataset with positive and negative invoice values, how would you convert positive values to negative while keeping the negative values unchnged?
how can you convert date like "20/04/1963" to integer format
given a dataset containing alphanumeric values and integer , how would you extract specific alphanumeric sequence like "ML","GL","LTR" and create a new dataframe to view only these sequences in spark?
what kind of questions have you encountered related to data modelling in your project ?

