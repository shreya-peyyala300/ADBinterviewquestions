1)Spark internal Architecture?

2)Repartition and colease

3)Cache and Persist

4)Difference between CSV JSON Parquest and Avro

5)Driver and Executor OOM

6)groupby key vs reduce by key. Which one is optimal and why ?

7)Transformation(filter,union) and Action(showw,collect,take,count,save) 

8)External Shuffle Service why is it used ?

8)Dynamic Resource Allocation why do we need Shuffle service ?

9)Spark App lifecycle->Spark submit->Driver Initiates Spark session->DAG creates logical plan ->Task Executor requests cluster manager for resources->Cluster manager allocates the resources -> Driver establishes connection with worker and assigns task->Worker executes the task and returs result to the driver ->driver returns result to the User ->App end 

10)lineage and DAG

11)catalyst Optimization

12)RDD dataframe vs Dataset

13)Driver and Worker Process ? These are JVM Process , one worker node has many executors ,each executor runs its own JVM Process

14)Spark submit ? -submit spark job
spark-submit --master Yarn --deployment-mode Client --executor-memory 4g --driver-memory 3g --num-executors 10 <.py>
local machine--spark-submit \
  --master local[4] \
  --executor-memory 2g \
  my_script.py

15)Application? Single spark code or multiple notebooks with complex logic

16)job-after app submit . Driver converts code to Job

17)stage - Job are divided into stages , based on the wide transform shuffle . For every shuffle a new stage is created 

18)Task - each task process 1 partition at a time . no of partition determine number of task 

19)RDD 

20)DAG -- keeps track of all transformation . For each transformation logical plan is created and lineage graph is maintained by dag

21)Executor - can be config by spark settings 

22)partition
23)core - threads

24)on heap memory -the executor memory on the jvm ,managed by jvm

25)off heap memory -the executor memory lies outside Jvm process manages by os

26)ibraries supported by spark -sql,stream,ml,graphx 

27)Driver architecture --tf->Dag scheduler ->Task Scheduler->Cluster Manager ->Deploye Scheduler to workers ->worker result given to driver tracked by out put tracker 

28) Worker Node Architecture ->executor -> cores->onheap memory (only one per executor) 
 all the above also connects to off heap memeory 

29)Advantage of ooff heap memory? we donot need to serialize and deserialize  ..........  Garbage collector -when memory full, gc scans entire memory and removes obsolute objects . This is unecessary process . This is present in off heap memory 

30)On heap memory architecture --see the image.png

31)calculate the number of cores and memory required (ex- 6 machines , 16 core , 64GB Memory)

32)Narrow(No shuffle,faster and efficient)-map,filter,union,mappartitiions,sample vs Wide Transformation (shuffle happens group by key , reduce by key ,join)

33)Explain Storage layer of onheap memory
Reserver Memory-Reserved by spark for internal purpose 
user Memory-storing the data structures created and managed by users code
Execution memory-jvm heap use full for shuffle operations
Storage Memory-JVM Heap for cache data 
dis: take more time to execute as gc will also pause the process to remove the objects

34)Explain off heap memory
if onheap cannot accomodate take help of off heap 
enable off heap by spark.memory.offHeap.use to true 
Accessing off heap is slower than the on heap but faster than read/write from disk .
Garbage collection scan is avoided .

(young generation move data to->old generation )

35)Cluster?
Databricks cluster is a set of computational resources and configurations on which you run DE, DS and Da workloads such as ETL Pipelines ,Stream Analytics and adhoc ml models .
present in compute . can conf min and max worker nodes
36)types of cluster 
All purpose CLusters:
used for developement . we can see output interactively 
Terminated ,restared manually.
Computing resource is share .

Job Cluster:
can run fast and robust automated jobs .Created automatically and terminates after end of excution 
Use for scheduled jobs

pools:
predefine the node count when attaching the cluster .
less booting time . Suitable for cost saving and larger teams .

37)Cluster modes ?
Standard:suitable for single user and if no team collaboration is required 
High Concurrency: suitable for collaboration. Provides fine grain sharing for max resources and min latency
Single Node: this mode runs job only on driver node and no worker node are provisioned .

38)Cluster runtime?
Runtime is set of core components that are needed for cluster to run 

39)spot instance?
workload of less priority. Azure check free instance(lesscost) uses this

40)Worker Types and driver type?
compute optimize
general 
memory
storage optimized

Cluster features :
logging into dbfs
tag
spark config

41)Reading CSV File
df=spark.read.format("")
.option("inferSchema","true")
.option("header","true")
.option("sep",",")
.schema(schema)
.load(path1,path2)

42)what is inferSchema?
Automatic Type Detection: It can automatically infer the data types of columns such as String, Integer, Double, Date, etc.
Costly Operation
Default Behavior: By default, Spark assumes that the data is of type String for each column if inferSchema is not enabled.

43)what is printSchema()?syntax

44)crud:
add:df.withColumn("new_column_name",Value)
rename: df.withColumnRenamed("Oldname","New_name")
drop:df.drop("column Name")

45)Add new Column:
df.withColumn("location",lit("Mumbai")).show()

46)How to add newColumn by calculation
df.withColumn("Bonus",emDf.salary*0.1)

47)join
# Joining on multiple columns
df1 = spark.createDataFrame([(1, "Alice", "A"), (2, "Bob", "B"), (3, "Charlie", "C")], ["id", "name", "group"])
df2 = spark.createDataFrame([(1, "Math", "A"), (2, "Science", "B"), (4, "History", "D")], ["id", "subject", "group"])

result = df1.join(df2, on=["id", "group"], how="inner")
result.show()

from pyspark.sql.functions import broadcast
result = df1.join(broadcast(df2), on="id", how="inner")

Join Type	Description
inner	Returns only matching rows from both DataFrames (default join type).
left	Returns all rows from the left DataFrame, matching rows from the right.
right	Returns all rows from the right DataFrame, matching rows from the left.
outer	Returns all rows from both DataFrames, with null values where no match.
left_semi	Returns only rows from the left DataFrame that have a matching key in the right.
left_anti	Returns only rows from the left DataFrame that do not have a matching key in the right.

48)DBUtils-
Databricks Utils help to interact with databricks file and Notebooks.
handle i/p and o/p of notebooks

dbutils.fs.help()
dbutils.notebook.help()
dbutils.widgets.help()
dbutils.secrets.help("get")
dbutils.fs.help("cp")
dbutils.notebook.help("exit")


49)Dynamic Partition Pruning / Predicate push down 
50)Handling Skewed data
51)Spark Optimizations 
52)Partitio Vs Buckets
53) workspace in ADB 
54)Assets of Workspace in ADB 
55) how to access work space?
56)notebooks ?
57)Job ? Job is a mechanism to run code in ADB  we can automate this 
58)library?
59)Data in ADB?
60)Experiments?
61)Workflows?
Azure Databricks (ADB), a workflow refers to the process of organizing and executing a series of tasks, notebooks, or jobs that are intended to accomplish a particular data engineering, data science, or machine learning process.
Databricks provides several tools to help define, schedule, and automate workflows efficiently, from simple batch processing to complex data pipelines.

Example of a Databricks Workflow
Consider a simple data pipeline where you:

Extract data from a source (e.g., a CSV file in Azure Blob Storage).
Transform the data using a series of PySpark transformations.
Load the transformed data into a Delta Lake table for further processing.
This process can be broken down into several tasks:

Task 1: Run a notebook that extracts data from Azure Blob Storage and performs some basic cleaning.
Task 2: Run a second notebook to perform more complex transformations on the data.
Task 3: Save the transformed data to Delta Lake, ensuring that data integrity is maintained.
In the job configuration, you can set the task dependencies so that Task 2 will not start until Task 1 finishes, and Task 3 will start only after Task 2.

62)Folder ?static assets 
63)special folder ? worspace shared user, cannot move or rename a folder 
64)flatten json:
df=spark.read.option("multiline","true").json("path")
65)


